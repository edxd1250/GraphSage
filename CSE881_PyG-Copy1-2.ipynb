{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "adj = sp.load_npz('./data_2024/adj.npz')\n",
    "feat  = np.load('./data_2024/features.npy')\n",
    "labels = np.load('./data_2024/labels.npy')\n",
    "splits = json.load(open('./data_2024/splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "n_components = 256\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_feat = pca.fit_transform(feat)\n",
    "\n",
    "\n",
    "# Converting the reduced features and other arrays to torch tensors\n",
    "reduced_feat = torch.tensor(reduced_feat, dtype=torch.float)\n",
    "full_labels = -1 * np.ones(shape=(reduced_feat.shape[0],), dtype=np.int64)\n",
    "full_labels[idx_train] = labels\n",
    "labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj)\n",
    "\n",
    "# Converting numpy arrays to torch tensors\n",
    "# feat = torch.tensor(feat, dtype=torch.float)\n",
    "# full_labels = -1 * np.ones(shape=(feat.shape[0],), dtype=np.int64)\n",
    "# full_labels[idx_train] = labels\n",
    "# labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "data = Data(x=reduced_feat, edge_index=edge_index, y=labels)\n",
    "# data = Data(x=feat, edge_index=edge_index, y=labels)\n",
    "\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[idx_train] = True\n",
    "test_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "idx_train =idx_train + idx_test\n",
    "train_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.train_mask.tolist().count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2480, 256], edge_index=[2, 10100], y=[2480], train_mask=[2480], test_mask=[2480])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, num_hidden)\n",
    "        self.hid1 = GCNConv(num_hidden, 16)\n",
    "        self.hid2 = GCNConv(16, num_hidden)\n",
    "        self.conv2 = GCNConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.hid1(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.hid2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 0: Train Loss: 1.9373828172683716, Val Loss: 1.9410227537155151, Val Acc: 0.3000\n",
      "Epoch 10: Train Loss: 0.39241471886634827, Val Loss: 0.8774828910827637, Val Acc: 0.8400\n",
      "Epoch 20: Train Loss: 0.1726866066455841, Val Loss: 0.7101258635520935, Val Acc: 0.8600\n",
      "Epoch 30: Train Loss: 0.10583116114139557, Val Loss: 1.1731590032577515, Val Acc: 0.8200\n",
      "Epoch 40: Train Loss: 0.1497054100036621, Val Loss: 0.5822405815124512, Val Acc: 0.8400\n",
      "Epoch 50: Train Loss: 0.05215824767947197, Val Loss: 1.2186845541000366, Val Acc: 0.7800\n",
      "Epoch 60: Train Loss: 0.04691096022725105, Val Loss: 1.3242735862731934, Val Acc: 0.8000\n",
      "Epoch 70: Train Loss: 0.04846368357539177, Val Loss: 1.4133754968643188, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.058618318289518356, Val Loss: 1.4088554382324219, Val Acc: 0.7800\n",
      "Epoch 90: Train Loss: 0.05487129092216492, Val Loss: 2.0155463218688965, Val Acc: 0.7600\n",
      "Epoch 100: Train Loss: 0.0745745450258255, Val Loss: 1.0297975540161133, Val Acc: 0.7800\n",
      "Epoch 110: Train Loss: 0.055260833352804184, Val Loss: 1.3844215869903564, Val Acc: 0.8000\n",
      "Epoch 120: Train Loss: 0.04127250239253044, Val Loss: 1.7549899816513062, Val Acc: 0.7600\n",
      "Epoch 130: Train Loss: 0.02893689274787903, Val Loss: 2.0551345348358154, Val Acc: 0.7800\n",
      "Epoch 140: Train Loss: 0.04186874255537987, Val Loss: 1.1887017488479614, Val Acc: 0.7800\n",
      "Epoch 150: Train Loss: 0.04905588552355766, Val Loss: 1.603445291519165, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.029828818514943123, Val Loss: 1.3736926317214966, Val Acc: 0.7600\n",
      "Epoch 170: Train Loss: 0.02498357743024826, Val Loss: 1.009223222732544, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.05610266700387001, Val Loss: 2.772073268890381, Val Acc: 0.7800\n",
      "Epoch 190: Train Loss: 0.07615137845277786, Val Loss: 1.5670644044876099, Val Acc: 0.8000\n",
      "Epoch 200: Train Loss: 0.03758193179965019, Val Loss: 1.2315932512283325, Val Acc: 0.8400\n",
      "Epoch 210: Train Loss: 0.038320302963256836, Val Loss: 1.235586166381836, Val Acc: 0.8200\n",
      "Epoch 220: Train Loss: 0.035876624286174774, Val Loss: 1.7520755529403687, Val Acc: 0.7800\n",
      "Epoch 230: Train Loss: 0.05563965067267418, Val Loss: 1.4078123569488525, Val Acc: 0.8200\n",
      "Epoch 240: Train Loss: 0.03134974464774132, Val Loss: 0.9640775322914124, Val Acc: 0.8200\n",
      "Fold 2/10\n",
      "Epoch 0: Train Loss: 1.9530997276306152, Val Loss: 1.9266388416290283, Val Acc: 0.3200\n",
      "Epoch 10: Train Loss: 0.36110156774520874, Val Loss: 1.541244387626648, Val Acc: 0.7600\n",
      "Epoch 20: Train Loss: 0.14730580151081085, Val Loss: 2.093972682952881, Val Acc: 0.7200\n",
      "Epoch 30: Train Loss: 0.06809550523757935, Val Loss: 3.280834436416626, Val Acc: 0.7600\n",
      "Epoch 40: Train Loss: 0.06842771172523499, Val Loss: 3.517575263977051, Val Acc: 0.7800\n",
      "Epoch 50: Train Loss: 0.05612136423587799, Val Loss: 3.241276502609253, Val Acc: 0.7600\n",
      "Epoch 60: Train Loss: 0.029187893494963646, Val Loss: 2.4082446098327637, Val Acc: 0.7400\n",
      "Epoch 70: Train Loss: 0.051066119223833084, Val Loss: 2.3101534843444824, Val Acc: 0.7800\n",
      "Epoch 80: Train Loss: 0.038905393332242966, Val Loss: 2.2966959476470947, Val Acc: 0.7800\n",
      "Epoch 90: Train Loss: 0.044292472302913666, Val Loss: 2.399035692214966, Val Acc: 0.7600\n",
      "Epoch 100: Train Loss: 0.038461290299892426, Val Loss: 2.3524420261383057, Val Acc: 0.7600\n",
      "Epoch 110: Train Loss: 0.033973436802625656, Val Loss: 2.4184491634368896, Val Acc: 0.8000\n",
      "Epoch 120: Train Loss: 0.02328837476670742, Val Loss: 2.1054022312164307, Val Acc: 0.8000\n",
      "Epoch 130: Train Loss: 0.02783559449017048, Val Loss: 2.034444570541382, Val Acc: 0.7800\n",
      "Epoch 140: Train Loss: 0.053439781069755554, Val Loss: 2.1494481563568115, Val Acc: 0.7800\n",
      "Epoch 150: Train Loss: 0.05202525109052658, Val Loss: 2.092273235321045, Val Acc: 0.8200\n",
      "Epoch 160: Train Loss: 0.027899296954274178, Val Loss: 2.3950822353363037, Val Acc: 0.8000\n",
      "Epoch 170: Train Loss: 0.02819996513426304, Val Loss: 2.796189785003662, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.03977220878005028, Val Loss: 2.679511070251465, Val Acc: 0.8200\n",
      "Epoch 190: Train Loss: 0.02328636683523655, Val Loss: 2.0830538272857666, Val Acc: 0.8000\n",
      "Epoch 200: Train Loss: 0.03937486559152603, Val Loss: 2.3149566650390625, Val Acc: 0.8000\n",
      "Epoch 210: Train Loss: 0.024482838809490204, Val Loss: 2.7357351779937744, Val Acc: 0.7600\n",
      "Epoch 220: Train Loss: 0.026672039180994034, Val Loss: 2.3218984603881836, Val Acc: 0.7600\n",
      "Epoch 230: Train Loss: 0.042683277279138565, Val Loss: 2.5558156967163086, Val Acc: 0.7600\n",
      "Epoch 240: Train Loss: 0.02894097939133644, Val Loss: 3.187058925628662, Val Acc: 0.7600\n",
      "Fold 3/10\n",
      "Epoch 0: Train Loss: 1.9589165449142456, Val Loss: 1.9617117643356323, Val Acc: 0.1400\n",
      "Epoch 10: Train Loss: 0.4414973855018616, Val Loss: 1.2580817937850952, Val Acc: 0.7800\n",
      "Epoch 20: Train Loss: 0.2011764496564865, Val Loss: 3.085667610168457, Val Acc: 0.7600\n",
      "Epoch 30: Train Loss: 0.12233590334653854, Val Loss: 2.022002935409546, Val Acc: 0.8200\n",
      "Epoch 40: Train Loss: 0.08025207370519638, Val Loss: 1.7094292640686035, Val Acc: 0.8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     31\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid1(x, edge_index)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid2(x, edge_index)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate.py:230\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    221\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    222\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    223\u001b[0m                 edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(\n\u001b[1;32m    231\u001b[0m     out,\n\u001b[1;32m    232\u001b[0m     index\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mindex,\n\u001b[1;32m    233\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mptr,\n\u001b[1;32m    234\u001b[0m     dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:612\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    597\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    602\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_module(inputs, index, ptr\u001b[38;5;241m=\u001b[39mptr, dim_size\u001b[38;5;241m=\u001b[39mdim_size,\n\u001b[1;32m    613\u001b[0m                             dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:128\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, index\u001b[38;5;241m=\u001b[39mindex, ptr\u001b[38;5;241m=\u001b[39mptr, dim_size\u001b[38;5;241m=\u001b[39mdim_size,\n\u001b[1;32m    129\u001b[0m                             dim\u001b[38;5;241m=\u001b[39mdim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(x, index, ptr, dim_size, dim, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:182\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scatter(x, index, dim, dim_size, reduce)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 10\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "\n",
    "    model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.09, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(250):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.84:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gcn_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 0: Train Loss: 1.9459551572799683, Val Loss: 1.9481924772262573, Val Acc: 0.5800\n",
      "Epoch 50: Train Loss: 0.08833102881908417, Val Loss: 0.7390847206115723, Val Acc: 0.8400\n",
      "Epoch 100: Train Loss: 0.09454435855150223, Val Loss: 0.5776011943817139, Val Acc: 0.8400\n",
      "Epoch 150: Train Loss: 0.06947901844978333, Val Loss: 0.6622166633605957, Val Acc: 0.9000\n",
      "Fold 2/10\n",
      "Epoch 0: Train Loss: 1.9357792139053345, Val Loss: 1.9523398876190186, Val Acc: 0.5000\n",
      "Epoch 50: Train Loss: 0.11293189972639084, Val Loss: 1.430882215499878, Val Acc: 0.7400\n",
      "Epoch 100: Train Loss: 0.06969195604324341, Val Loss: 1.5973516702651978, Val Acc: 0.7200\n",
      "Epoch 150: Train Loss: 0.06445493549108505, Val Loss: 1.8129699230194092, Val Acc: 0.7400\n",
      "Fold 3/10\n",
      "Epoch 0: Train Loss: 1.9636907577514648, Val Loss: 1.9586172103881836, Val Acc: 0.5600\n",
      "Epoch 50: Train Loss: 0.10136140882968903, Val Loss: 1.3195126056671143, Val Acc: 0.7600\n",
      "Epoch 100: Train Loss: 0.07852602750062943, Val Loss: 1.0999146699905396, Val Acc: 0.7400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m---> 57\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     58\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes, heads=12, output_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, num_hidden, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(num_hidden*heads, num_classes, heads=output_heads, concat=False, dropout=0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Graph Attention Layer\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second Graph Attention Layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.86:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gat_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Epoch 0: Train Loss: 1.9196665287017822, Val Loss: 1.9170328378677368, Val Acc: 0.4286\n",
      "Epoch 50: Train Loss: 0.0027849439065903425, Val Loss: 0.5366899371147156, Val Acc: 0.8393\n",
      "Epoch 100: Train Loss: 0.00939025916159153, Val Loss: 0.5010973811149597, Val Acc: 0.8393\n",
      "Epoch 150: Train Loss: 0.007293348200619221, Val Loss: 0.49514517188072205, Val Acc: 0.8393\n",
      "Fold 2/9\n",
      "Epoch 0: Train Loss: 1.9309641122817993, Val Loss: 1.931994915008545, Val Acc: 0.4545\n",
      "Epoch 50: Train Loss: 0.003553807269781828, Val Loss: 0.9710485935211182, Val Acc: 0.7636\n",
      "Epoch 100: Train Loss: 0.008319416083395481, Val Loss: 0.7978211641311646, Val Acc: 0.7091\n",
      "Epoch 150: Train Loss: 0.00715416157618165, Val Loss: 0.8832887411117554, Val Acc: 0.6727\n",
      "Fold 3/9\n",
      "Epoch 0: Train Loss: 1.943326711654663, Val Loss: 1.9446052312850952, Val Acc: 0.4727\n",
      "Epoch 50: Train Loss: 0.0032829379197210073, Val Loss: 0.8310878872871399, Val Acc: 0.7818\n",
      "Epoch 100: Train Loss: 0.008050698786973953, Val Loss: 0.7070969343185425, Val Acc: 0.8000\n",
      "Epoch 150: Train Loss: 0.007471811026334763, Val Loss: 0.687676191329956, Val Acc: 0.8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     53\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 54\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m, in \u001b[0;36mGraphSAGE.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Second GraphSAGE Layer\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/sage_conv.py:134\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mrelu(), x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    135\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l(out)\n\u001b[1;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate.py:158\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m    152\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    153\u001b[0m         out,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(\n\u001b[1;32m    159\u001b[0m         edge_index,\n\u001b[1;32m    160\u001b[0m         x,\n\u001b[1;32m    161\u001b[0m         mutable_size,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate.py:76\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m0\u001b[39m, _x_0)\n\u001b[0;32m---> 76\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select(_x_0, edge_index_j)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:300\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select_safe(src, index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:304\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_index_select_safe\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, index: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, num_hidden)\n",
    "        self.conv2 = SAGEConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GraphSAGE Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second GraphSAGE Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 9\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.85:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gsage_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking GCN, GAT & GSage Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/p6mr7_3s7xq258wywpvvwhn40000gn/T/ipykernel_44506/4168977634.py:44: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_votes, _ = mode(all_predictions, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import mode\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = (data.y.max() + 1).item() \n",
    "\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "model_types = ['gcn', 'gat', 'gsage'] \n",
    "num_folds = 10\n",
    "\n",
    "for model_type in model_types:\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        if model_type == 'gcn':\n",
    "            model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        elif model_type == 'gat':\n",
    "            model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        elif model_type == 'gsage':\n",
    "            model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "            \n",
    "        try:\n",
    "            model_path = f'./{model_type}_best_fold_{fold}.pt' \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(data.to(device))\n",
    "                preds = out.argmax(dim=1)\n",
    "                all_predictions.append(preds.cpu().numpy())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "majority_votes, _ = mode(all_predictions, axis=0)\n",
    "majority_votes = torch.tensor(majority_votes.squeeze(), dtype=torch.long)\n",
    "correct = (majority_votes[data.val_mask] == data.y[data.val_mask]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# pred = model(data).argmax(dim=1)\n",
    "# correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "# acc = int(correct) / int(data.val_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submitting the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = majority_votes[idx_test]\n",
    "np.savetxt('submission.txt', preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "605cf7bcaa80724d107f41962e778124ae33e6c6d0f7e8df414b9d839a160efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
