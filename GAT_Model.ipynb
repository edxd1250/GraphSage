{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 881 Project\n",
    "**Group Names: Edmond Anderson, Sarah Bradford, Lacey Hamilton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from scipy import sparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data_2024')\n",
    "adj_matrix = sparse.load_npz(path/'adj.npz')\n",
    "feat  = np.load(path/'features.npy')\n",
    "labels = np.load(path/'labels.npy')\n",
    "splits = json.load(open(path/'splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adj_matrix \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39mcsr_matrix((adj[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], adj[\u001b[39m'\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m'\u001b[39m], adj[\u001b[39m'\u001b[39m\u001b[39mindptr\u001b[39m\u001b[39m'\u001b[39m]), shape\u001b[39m=\u001b[39madj[\u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adj' is not defined"
     ]
    }
   ],
   "source": [
    "adj_matrix = sparse.csr_matrix((adj['data'], adj['indices'], adj['indptr']), shape=adj['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = torch.FloatTensor(adj_matrix.toarray())\n",
    "node_features = torch.FloatTensor(feat)\n",
    "labels = torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, adj_matrix, node_features, labels):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.node_features = node_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.adj_matrix[idx], self.node_features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx_train', 'idx_test'])\n"
     ]
    }
   ],
   "source": [
    "print(splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = splits['idx_train']\n",
    "idx_test = splits['idx_test']\n",
    "idx_train = [idx for idx in idx_train if idx < len(labels)]\n",
    "idx_test = [idx for idx in idx_test if idx < len(labels)]\n",
    "# idx_train.sort()\n",
    "# idx_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphDataset(adj_matrix[idx_train], node_features[idx_train], labels[idx_train])\n",
    "test_dataset = GraphDataset(adj_matrix[idx_test], node_features[idx_test], labels[idx_test])\n",
    "#val_dataset = GraphDataset(adj_matrix[idx_val], node_features[idx_val], labels[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the GAT layer\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.attn_fc = nn.Linear(2 * out_features, 1)\n",
    "\n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        h = self.fc(node_features)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.zeros(N, N)\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if adj_matrix[i, j] == 1:\n",
    "                    attn_input = torch.cat([h[i], h[j]], dim=0)\n",
    "                    attn_scores[i, j] = self.attn_fc(attn_input).squeeze()\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        attn_coefficients = nn.functional.softmax(attn_scores, dim=1)\n",
    "\n",
    "        # Compute output features\n",
    "        h_prime = torch.zeros(N, h.size()[1])\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                h_prime[i] += attn_coefficients[i, j] * h[j]\n",
    "\n",
    "        return h_prime\n",
    "# defining th GAT model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GATLayer(in_features, hidden_features)\n",
    "        self.layer2 = GATLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        h = self.layer1(adj_matrix, node_features)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer2(adj_matrix, h)\n",
    "        return h\n",
    "# GAT model\n",
    "model = GAT(in_features=node_features.size()[1], hidden_features=8, out_features=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.895\n",
      "[2] loss: 1.478\n",
      "[3] loss: 1.005\n",
      "[4] loss: 0.588\n",
      "[5] loss: 0.374\n",
      "[6] loss: 0.278\n",
      "[7] loss: 0.234\n",
      "[8] loss: 0.203\n",
      "[9] loss: 0.188\n",
      "[10] loss: 0.169\n",
      "[11] loss: 0.168\n",
      "[12] loss: 0.120\n",
      "[13] loss: 0.103\n",
      "[14] loss: 0.079\n",
      "[15] loss: 0.080\n",
      "[16] loss: 0.069\n",
      "[17] loss: 0.065\n",
      "[18] loss: 0.066\n",
      "[19] loss: 0.055\n",
      "[20] loss: 0.052\n",
      "[21] loss: 0.062\n",
      "[22] loss: 0.033\n",
      "[23] loss: 0.035\n",
      "[24] loss: 0.032\n",
      "[25] loss: 0.038\n",
      "[26] loss: 0.029\n",
      "[27] loss: 0.025\n",
      "[28] loss: 0.023\n",
      "[29] loss: 0.022\n",
      "[30] loss: 0.022\n",
      "[31] loss: 0.021\n",
      "[32] loss: 0.023\n",
      "[33] loss: 0.020\n",
      "[34] loss: 0.018\n",
      "[35] loss: 0.023\n",
      "[36] loss: 0.027\n",
      "[37] loss: 0.017\n",
      "[38] loss: 0.013\n",
      "[39] loss: 0.013\n",
      "[40] loss: 0.012\n",
      "[41] loss: 0.012\n",
      "[42] loss: 0.011\n",
      "[43] loss: 0.011\n",
      "[44] loss: 0.010\n",
      "[45] loss: 0.010\n",
      "[46] loss: 0.009\n",
      "[47] loss: 0.009\n",
      "[48] loss: 0.008\n",
      "[49] loss: 0.008\n",
      "[50] loss: 0.008\n",
      "Accuracy: 24 %\n"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        adj_matrix, node_features, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(adj_matrix, node_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "# model testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        adj_matrix, node_features, labels = data\n",
    "        outputs = model(adj_matrix, node_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.877\n",
      "[2] loss: 1.544\n",
      "[3] loss: 1.101\n",
      "[4] loss: 0.814\n",
      "[5] loss: 0.733\n",
      "[6] loss: 0.659\n",
      "[7] loss: 0.548\n",
      "[8] loss: 0.468\n",
      "[9] loss: 0.454\n",
      "[10] loss: 0.487\n",
      "[11] loss: 0.378\n",
      "[12] loss: 0.332\n",
      "[13] loss: 0.301\n",
      "[14] loss: 0.268\n",
      "[15] loss: 0.239\n",
      "[16] loss: 0.228\n",
      "[17] loss: 0.222\n",
      "[18] loss: 0.228\n",
      "[19] loss: 0.225\n",
      "[20] loss: 0.234\n",
      "[21] loss: 0.242\n",
      "[22] loss: 0.222\n",
      "[23] loss: 0.222\n",
      "[24] loss: 0.220\n",
      "[25] loss: 0.220\n",
      "[26] loss: 0.221\n",
      "[27] loss: 0.217\n",
      "[28] loss: 0.220\n",
      "[29] loss: 0.221\n",
      "[30] loss: 0.217\n",
      "[31] loss: 0.234\n",
      "[32] loss: 0.314\n",
      "[33] loss: 0.452\n",
      "[34] loss: 0.204\n",
      "[35] loss: 0.198\n",
      "[36] loss: 0.199\n",
      "[37] loss: 0.189\n",
      "[38] loss: 0.186\n",
      "[39] loss: 0.192\n",
      "[40] loss: 0.191\n",
      "[41] loss: 0.186\n",
      "[42] loss: 0.184\n",
      "[43] loss: 0.185\n",
      "[44] loss: 0.182\n",
      "[45] loss: 0.182\n",
      "[46] loss: 0.187\n",
      "[47] loss: 0.184\n",
      "[48] loss: 0.181\n",
      "[49] loss: 0.184\n",
      "[50] loss: 0.181\n",
      "[51] loss: 0.181\n",
      "[52] loss: 0.181\n",
      "[53] loss: 0.182\n",
      "[54] loss: 0.263\n",
      "[55] loss: 0.185\n",
      "[56] loss: 0.181\n",
      "[57] loss: 0.180\n",
      "[58] loss: 0.183\n",
      "[59] loss: 0.185\n",
      "[60] loss: 0.180\n",
      "[61] loss: 0.183\n",
      "[62] loss: 0.183\n",
      "[63] loss: 0.183\n",
      "[64] loss: 0.180\n",
      "[65] loss: 0.182\n",
      "[66] loss: 0.181\n",
      "[67] loss: 0.182\n",
      "[68] loss: 0.183\n",
      "[69] loss: 0.183\n",
      "[70] loss: 0.179\n",
      "[71] loss: 0.182\n",
      "[72] loss: 0.181\n",
      "[73] loss: 0.180\n",
      "[74] loss: 0.181\n",
      "[75] loss: 0.179\n",
      "[76] loss: 0.181\n",
      "[77] loss: 0.180\n",
      "[78] loss: 0.182\n",
      "[79] loss: 0.179\n",
      "[80] loss: 0.179\n",
      "[81] loss: 0.182\n",
      "[82] loss: 0.181\n",
      "[83] loss: 0.180\n",
      "[84] loss: 0.181\n",
      "[85] loss: 0.180\n",
      "[86] loss: 0.179\n",
      "[87] loss: 0.180\n",
      "[88] loss: 0.182\n",
      "[89] loss: 0.180\n",
      "[90] loss: 0.178\n",
      "[91] loss: 0.182\n",
      "[92] loss: 0.182\n",
      "[93] loss: 0.176\n",
      "[94] loss: 0.180\n",
      "[95] loss: 0.181\n",
      "[96] loss: 0.178\n",
      "[97] loss: 0.179\n",
      "[98] loss: 0.179\n",
      "[99] loss: 0.179\n",
      "[100] loss: 0.179\n",
      "Accuracy: 18 %\n"
     ]
    }
   ],
   "source": [
    "# improve the accuracy by adding more layers\n",
    "class ImprovedGAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(ImprovedGAT, self).__init__()\n",
    "        self.layer1 = GATLayer(in_features, hidden_features)\n",
    "        self.layer2 = GATLayer(hidden_features, hidden_features)\n",
    "        self.layer3 = GATLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, adj_matrix, node_features):\n",
    "        h = self.layer1(adj_matrix, node_features)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer2(adj_matrix, h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer3(adj_matrix, h)\n",
    "        return h\n",
    "# Improved GAT model\n",
    "model = ImprovedGAT(in_features=node_features.size()[1], hidden_features=8, out_features=7)\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        adj_matrix, node_features, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(adj_matrix, node_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "# model testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        adj_matrix, node_features, labels = data\n",
    "        outputs = model(adj_matrix, node_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(test_loader,node_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mImprovedGAT.forward\u001b[0;34m(self, adj_matrix, node_features)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, adj_matrix, node_features):\n\u001b[0;32m---> 10\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(adj_matrix, node_features)\n\u001b[1;32m     11\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m     12\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(adj_matrix, h)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mGATLayer.forward\u001b[0;34m(self, adj_matrix, node_features)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m adj_matrix[i, j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     17\u001b[0m             attn_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h[i], h[j]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m             attn_scores[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_fc(attn_input)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pred = model(test_loader,node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m[idx_test]\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, preds, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "preds = pred[idx_test]\n",
    "np.savetxt('submission.txt', preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('graphsage')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1cd7fe2092f8558eaeec7a344d6c72cd3491c14a7d06f3a2798a65abb497b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
