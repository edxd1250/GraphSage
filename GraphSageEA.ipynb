{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218fe1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "path = Path('data_2024')\n",
    "adj = sp.load_npz(path/'adj.npz')\n",
    "feat  = np.load(path/'features.npy')\n",
    "labels = np.load(path/'labels.npy')\n",
    "splits = json.load(open(path/'splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7555ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2119,\n",
       " 2206,\n",
       " 2165,\n",
       " 1994,\n",
       " 520,\n",
       " 521,\n",
       " 2103,\n",
       " 1838,\n",
       " 1840,\n",
       " 116,\n",
       " 134,\n",
       " 2477,\n",
       " 1522,\n",
       " 821,\n",
       " 1393,\n",
       " 1983,\n",
       " 2136,\n",
       " 2478,\n",
       " 1182,\n",
       " 1617,\n",
       " 899,\n",
       " 1898,\n",
       " 1592,\n",
       " 903,\n",
       " 815,\n",
       " 1395,\n",
       " 871,\n",
       " 1596,\n",
       " 1188,\n",
       " 513,\n",
       " 791,\n",
       " 1296,\n",
       " 1608,\n",
       " 1149,\n",
       " 1568,\n",
       " 1625,\n",
       " 1101,\n",
       " 829,\n",
       " 1595,\n",
       " 1912,\n",
       " 1856,\n",
       " 308,\n",
       " 603,\n",
       " 1122,\n",
       " 566,\n",
       " 2175,\n",
       " 2066,\n",
       " 1556,\n",
       " 339,\n",
       " 1264,\n",
       " 531,\n",
       " 2266,\n",
       " 3,\n",
       " 191,\n",
       " 423,\n",
       " 2054,\n",
       " 1098,\n",
       " 338,\n",
       " 2215,\n",
       " 2184,\n",
       " 302,\n",
       " 982,\n",
       " 725,\n",
       " 362,\n",
       " 1931,\n",
       " 2178,\n",
       " 745,\n",
       " 1248,\n",
       " 376,\n",
       " 171,\n",
       " 646,\n",
       " 1569,\n",
       " 1778,\n",
       " 1974,\n",
       " 321,\n",
       " 156,\n",
       " 2265,\n",
       " 1814,\n",
       " 1050,\n",
       " 868,\n",
       " 846,\n",
       " 1969,\n",
       " 2382,\n",
       " 1508,\n",
       " 636,\n",
       " 428,\n",
       " 2164,\n",
       " 1044,\n",
       " 1378,\n",
       " 2259,\n",
       " 1265,\n",
       " 809,\n",
       " 2387,\n",
       " 1056,\n",
       " 268,\n",
       " 1459,\n",
       " 1509,\n",
       " 1158,\n",
       " 1107,\n",
       " 1024,\n",
       " 755,\n",
       " 293,\n",
       " 2218,\n",
       " 194,\n",
       " 770,\n",
       " 661,\n",
       " 757,\n",
       " 1450,\n",
       " 1328,\n",
       " 1151,\n",
       " 986,\n",
       " 1256,\n",
       " 325,\n",
       " 827,\n",
       " 1300,\n",
       " 1890,\n",
       " 607,\n",
       " 76,\n",
       " 292,\n",
       " 2237,\n",
       " 236,\n",
       " 1474,\n",
       " 203,\n",
       " 1065,\n",
       " 2317,\n",
       " 724,\n",
       " 2308,\n",
       " 2236,\n",
       " 100,\n",
       " 1423,\n",
       " 1824,\n",
       " 140,\n",
       " 1641,\n",
       " 148,\n",
       " 1672,\n",
       " 1540,\n",
       " 240,\n",
       " 2145,\n",
       " 364,\n",
       " 1291,\n",
       " 921,\n",
       " 637,\n",
       " 1085,\n",
       " 1484,\n",
       " 139,\n",
       " 1587,\n",
       " 758,\n",
       " 974,\n",
       " 1257,\n",
       " 1476,\n",
       " 43,\n",
       " 878,\n",
       " 2263,\n",
       " 819,\n",
       " 391,\n",
       " 283,\n",
       " 1426,\n",
       " 1713,\n",
       " 1800,\n",
       " 2148,\n",
       " 701,\n",
       " 663,\n",
       " 2003,\n",
       " 1103,\n",
       " 1148,\n",
       " 658,\n",
       " 1124,\n",
       " 444,\n",
       " 1041,\n",
       " 1512,\n",
       " 618,\n",
       " 1701,\n",
       " 1465,\n",
       " 2471,\n",
       " 1644,\n",
       " 1609,\n",
       " 1620,\n",
       " 1268,\n",
       " 726,\n",
       " 2245,\n",
       " 2052,\n",
       " 48,\n",
       " 1271,\n",
       " 220,\n",
       " 1972,\n",
       " 2061,\n",
       " 1788,\n",
       " 1069,\n",
       " 1470,\n",
       " 402,\n",
       " 2284,\n",
       " 1783,\n",
       " 941,\n",
       " 2443,\n",
       " 1607,\n",
       " 535,\n",
       " 1278,\n",
       " 495,\n",
       " 1156,\n",
       " 694,\n",
       " 1092,\n",
       " 2114,\n",
       " 1934,\n",
       " 1458,\n",
       " 92,\n",
       " 1211,\n",
       " 1773,\n",
       " 1683,\n",
       " 381,\n",
       " 613,\n",
       " 73,\n",
       " 2059,\n",
       " 1981,\n",
       " 1571,\n",
       " 252,\n",
       " 1456,\n",
       " 1007,\n",
       " 639,\n",
       " 1175,\n",
       " 375,\n",
       " 1270,\n",
       " 59,\n",
       " 434,\n",
       " 2254,\n",
       " 1089,\n",
       " 763,\n",
       " 683,\n",
       " 2314,\n",
       " 1006,\n",
       " 2243,\n",
       " 1555,\n",
       " 417,\n",
       " 132,\n",
       " 397,\n",
       " 1254,\n",
       " 1468,\n",
       " 582,\n",
       " 807,\n",
       " 144,\n",
       " 2472,\n",
       " 731,\n",
       " 744,\n",
       " 1398,\n",
       " 1072,\n",
       " 1887,\n",
       " 638,\n",
       " 427,\n",
       " 1708,\n",
       " 479,\n",
       " 2410,\n",
       " 1746,\n",
       " 1993,\n",
       " 1591,\n",
       " 1369,\n",
       " 291,\n",
       " 2177,\n",
       " 1042,\n",
       " 58,\n",
       " 1014,\n",
       " 1396,\n",
       " 347,\n",
       " 1850,\n",
       " 2095,\n",
       " 89,\n",
       " 2258,\n",
       " 991,\n",
       " 52,\n",
       " 2077,\n",
       " 2005,\n",
       " 1100,\n",
       " 1980,\n",
       " 2089,\n",
       " 537,\n",
       " 713,\n",
       " 946,\n",
       " 814,\n",
       " 154,\n",
       " 84,\n",
       " 1252,\n",
       " 524,\n",
       " 1068,\n",
       " 1157,\n",
       " 1673,\n",
       " 87,\n",
       " 1570,\n",
       " 1605,\n",
       " 195,\n",
       " 817,\n",
       " 1299,\n",
       " 647,\n",
       " 1184,\n",
       " 559,\n",
       " 2372,\n",
       " 1311,\n",
       " 542,\n",
       " 540,\n",
       " 923,\n",
       " 1046,\n",
       " 424,\n",
       " 838,\n",
       " 2442,\n",
       " 940,\n",
       " 482,\n",
       " 259,\n",
       " 1769,\n",
       " 1452,\n",
       " 377,\n",
       " 928,\n",
       " 805,\n",
       " 2360,\n",
       " 1599,\n",
       " 1260,\n",
       " 501,\n",
       " 2042,\n",
       " 1843,\n",
       " 1187,\n",
       " 2446,\n",
       " 1970,\n",
       " 324,\n",
       " 2049,\n",
       " 1030,\n",
       " 305,\n",
       " 1377,\n",
       " 938,\n",
       " 687,\n",
       " 1541,\n",
       " 1751,\n",
       " 36,\n",
       " 1502,\n",
       " 2074,\n",
       " 2036,\n",
       " 712,\n",
       " 2475,\n",
       " 1749,\n",
       " 994,\n",
       " 722,\n",
       " 2204,\n",
       " 2301,\n",
       " 1709,\n",
       " 558,\n",
       " 1091,\n",
       " 164,\n",
       " 1391,\n",
       " 1245,\n",
       " 743,\n",
       " 2298,\n",
       " 463,\n",
       " 1885,\n",
       " 290,\n",
       " 1303,\n",
       " 984,\n",
       " 1926,\n",
       " 1590,\n",
       " 2015,\n",
       " 301,\n",
       " 1601,\n",
       " 1552,\n",
       " 1680,\n",
       " 631,\n",
       " 1386,\n",
       " 780,\n",
       " 879,\n",
       " 1962,\n",
       " 88,\n",
       " 1222,\n",
       " 1416,\n",
       " 920,\n",
       " 1925,\n",
       " 1497,\n",
       " 1837,\n",
       " 772,\n",
       " 95,\n",
       " 935,\n",
       " 1008,\n",
       " 2421,\n",
       " 13,\n",
       " 1059,\n",
       " 1422,\n",
       " 2094,\n",
       " 1623,\n",
       " 342,\n",
       " 1975,\n",
       " 1165,\n",
       " 792,\n",
       " 2239,\n",
       " 1281,\n",
       " 1204,\n",
       " 1879,\n",
       " 60,\n",
       " 1781,\n",
       " 1857,\n",
       " 1033,\n",
       " 1772,\n",
       " 173,\n",
       " 2451,\n",
       " 2273,\n",
       " 1665,\n",
       " 910,\n",
       " 404,\n",
       " 68,\n",
       " 750,\n",
       " 411,\n",
       " 795,\n",
       " 1564,\n",
       " 1294,\n",
       " 624,\n",
       " 1112,\n",
       " 2412,\n",
       " 1424,\n",
       " 416,\n",
       " 1792,\n",
       " 24,\n",
       " 500,\n",
       " 2344,\n",
       " 1735,\n",
       " 1135,\n",
       " 146,\n",
       " 1332,\n",
       " 2376,\n",
       " 1451,\n",
       " 852,\n",
       " 26,\n",
       " 2440,\n",
       " 1548,\n",
       " 2223,\n",
       " 1523,\n",
       " 802,\n",
       " 329,\n",
       " 734,\n",
       " 1888,\n",
       " 1553,\n",
       " 189,\n",
       " 300,\n",
       " 178,\n",
       " 12,\n",
       " 1408,\n",
       " 2252,\n",
       " 165,\n",
       " 65,\n",
       " 1734,\n",
       " 1093,\n",
       " 979,\n",
       " 564,\n",
       " 1104,\n",
       " 1383,\n",
       " 863,\n",
       " 1473,\n",
       " 793,\n",
       " 2321,\n",
       " 449,\n",
       " 1762,\n",
       " 1118,\n",
       " 1765,\n",
       " 1240,\n",
       " 1514,\n",
       " 2128,\n",
       " 1467,\n",
       " 1630,\n",
       " 1694,\n",
       " 1272,\n",
       " 387,\n",
       " 2151,\n",
       " 570,\n",
       " 1392,\n",
       " 528,\n",
       " 368,\n",
       " 889,\n",
       " 2182,\n",
       " 577,\n",
       " 2166,\n",
       " 554,\n",
       " 1075,\n",
       " 2426,\n",
       " 1073,\n",
       " 2203,\n",
       " 2436,\n",
       " 1639,\n",
       " 1525,\n",
       " 288,\n",
       " 749,\n",
       " 216,\n",
       " 1667,\n",
       " 30,\n",
       " 208,\n",
       " 1403,\n",
       " 1193,\n",
       " 1809,\n",
       " 1121,\n",
       " 1720,\n",
       " 2419,\n",
       " 2038,\n",
       " 1727,\n",
       " 708,\n",
       " 828,\n",
       " 769,\n",
       " 1666,\n",
       " 2170,\n",
       " 2441,\n",
       " 1127,\n",
       " 1320,\n",
       " 136,\n",
       " 1738,\n",
       " 1634,\n",
       " 2133,\n",
       " 739,\n",
       " 1128,\n",
       " 958,\n",
       " 2464,\n",
       " 1566,\n",
       " 1764,\n",
       " 118,\n",
       " 465,\n",
       " 1950,\n",
       " 1760,\n",
       " 837,\n",
       " 1967,\n",
       " 142,\n",
       " 1365,\n",
       " 925,\n",
       " 420,\n",
       " 1275,\n",
       " 782,\n",
       " 998,\n",
       " 255,\n",
       " 408,\n",
       " 1504,\n",
       " 1668,\n",
       " 1145,\n",
       " 496,\n",
       " 2407,\n",
       " 281,\n",
       " 2385,\n",
       " 1334,\n",
       " 1302,\n",
       " 719,\n",
       " 179,\n",
       " 1176,\n",
       " 1757,\n",
       " 1183,\n",
       " 662,\n",
       " 2037,\n",
       " 2233,\n",
       " 1600,\n",
       " 1285,\n",
       " 1785,\n",
       " 2016,\n",
       " 1138,\n",
       " 390,\n",
       " 2313,\n",
       " 1083,\n",
       " 235,\n",
       " 2116,\n",
       " 997,\n",
       " 201,\n",
       " 525,\n",
       " 2027,\n",
       " 232,\n",
       " 906,\n",
       " 1611,\n",
       " 1615,\n",
       " 1400,\n",
       " 1920,\n",
       " 1039,\n",
       " 2398,\n",
       " 2413,\n",
       " 1441,\n",
       " 1733,\n",
       " 1261,\n",
       " 109,\n",
       " 1126,\n",
       " 1870,\n",
       " 1200,\n",
       " 2046,\n",
       " 326,\n",
       " 2433,\n",
       " 320,\n",
       " 2140,\n",
       " 1340,\n",
       " 2009,\n",
       " 942,\n",
       " 2141,\n",
       " 1478,\n",
       " 1048,\n",
       " 595,\n",
       " 2332,\n",
       " 2004,\n",
       " 684,\n",
       " 660,\n",
       " 1040,\n",
       " 1172,\n",
       " 488,\n",
       " 504,\n",
       " 555,\n",
       " 1436,\n",
       " 1005,\n",
       " 1140,\n",
       " 760,\n",
       " 1952,\n",
       " 623,\n",
       " 930,\n",
       " 1938,\n",
       " 2159,\n",
       " 678,\n",
       " 1244,\n",
       " 2468,\n",
       " 1798,\n",
       " 2476,\n",
       " 1768,\n",
       " 63,\n",
       " 1808,\n",
       " 273,\n",
       " 967,\n",
       " 1544,\n",
       " 2288,\n",
       " 2363,\n",
       " 1711,\n",
       " 676,\n",
       " 2195,\n",
       " 1250,\n",
       " 2380,\n",
       " 1018,\n",
       " 672,\n",
       " 553,\n",
       " 222,\n",
       " 1821,\n",
       " 1621,\n",
       " 989,\n",
       " 1105,\n",
       " 860,\n",
       " 1796,\n",
       " 241,\n",
       " 1507,\n",
       " 1413,\n",
       " 1031,\n",
       " 936,\n",
       " 957,\n",
       " 137,\n",
       " 1045,\n",
       " 775,\n",
       " 1381,\n",
       " 839,\n",
       " 1804,\n",
       " 2147,\n",
       " 275,\n",
       " 2390,\n",
       " 2255,\n",
       " 262,\n",
       " 1917,\n",
       " 778,\n",
       " 2073,\n",
       " 2296,\n",
       " 2143,\n",
       " 918,\n",
       " 916,\n",
       " 1581,\n",
       " 1132,\n",
       " 659,\n",
       " 307,\n",
       " 61,\n",
       " 1859,\n",
       " 2322,\n",
       " 2469,\n",
       " 374,\n",
       " 669,\n",
       " 1196,\n",
       " 1987,\n",
       " 72,\n",
       " 953,\n",
       " 448,\n",
       " 166,\n",
       " 1614,\n",
       " 2256,\n",
       " 1603,\n",
       " 1226,\n",
       " 714,\n",
       " 1060,\n",
       " 353,\n",
       " 248,\n",
       " 1335,\n",
       " 1359,\n",
       " 665,\n",
       " 1446,\n",
       " 2323,\n",
       " 1488,\n",
       " 2430,\n",
       " 1900,\n",
       " 454,\n",
       " 57,\n",
       " 1763,\n",
       " 101,\n",
       " 949,\n",
       " 401,\n",
       " 238,\n",
       " 866,\n",
       " 718,\n",
       " 2112,\n",
       " 1054,\n",
       " 1520,\n",
       " 67,\n",
       " 441,\n",
       " 1223,\n",
       " 117,\n",
       " 1580,\n",
       " 27,\n",
       " 2458,\n",
       " 1219,\n",
       " 1394,\n",
       " 1518,\n",
       " 556,\n",
       " 344,\n",
       " 224,\n",
       " 1684,\n",
       " 869,\n",
       " 498,\n",
       " 698,\n",
       " 2474,\n",
       " 2219,\n",
       " 1930,\n",
       " 1020,\n",
       " 2343,\n",
       " 1632,\n",
       " 1940,\n",
       " 635,\n",
       " 1921,\n",
       " 1761,\n",
       " 1146,\n",
       " 1180,\n",
       " 2261,\n",
       " 877,\n",
       " 1954,\n",
       " 987,\n",
       " 1374,\n",
       " 168,\n",
       " 2386,\n",
       " 847,\n",
       " 1730,\n",
       " 299,\n",
       " 1361,\n",
       " 1531,\n",
       " 340,\n",
       " 352,\n",
       " 1388,\n",
       " 229,\n",
       " 851,\n",
       " 1471,\n",
       " 2088,\n",
       " 1207,\n",
       " 1448,\n",
       " 1013,\n",
       " 2109,\n",
       " 373,\n",
       " 2190,\n",
       " 573,\n",
       " 567,\n",
       " 108,\n",
       " 1739,\n",
       " 1854,\n",
       " 2424,\n",
       " 654,\n",
       " 93,\n",
       " 1262,\n",
       " 2194,\n",
       " 2179,\n",
       " 199,\n",
       " 1111,\n",
       " 1841,\n",
       " 1277,\n",
       " 1706,\n",
       " 50,\n",
       " 1533,\n",
       " 625,\n",
       " 1439,\n",
       " 2173,\n",
       " 1573,\n",
       " 1936,\n",
       " 1971,\n",
       " 1855,\n",
       " 162,\n",
       " 1649,\n",
       " 2356,\n",
       " 1025,\n",
       " 924,\n",
       " 192,\n",
       " 1700,\n",
       " 973,\n",
       " 1759,\n",
       " 1095,\n",
       " 97,\n",
       " 1412,\n",
       " 357,\n",
       " 2022,\n",
       " 1747,\n",
       " 1205,\n",
       " 703,\n",
       " 1883,\n",
       " 1779,\n",
       " 934,\n",
       " 1645,\n",
       " 243,\n",
       " 1119,\n",
       " 2262,\n",
       " 675,\n",
       " 615,\n",
       " 1437,\n",
       " 276,\n",
       " 1557,\n",
       " 1963,\n",
       " 2235,\n",
       " 2418,\n",
       " 1049,\n",
       " 1479,\n",
       " 271,\n",
       " 585,\n",
       " 1051,\n",
       " 1214,\n",
       " 2031,\n",
       " 1333,\n",
       " 1613,\n",
       " 530,\n",
       " 1891,\n",
       " 2053,\n",
       " 1102,\n",
       " 891,\n",
       " 227,\n",
       " 622,\n",
       " 2012,\n",
       " 2392,\n",
       " 1827,\n",
       " 1418,\n",
       " 1895,\n",
       " 1637,\n",
       " 1229,\n",
       " 1789,\n",
       " 1537,\n",
       " 2432,\n",
       " 1521,\n",
       " 1538,\n",
       " 1206,\n",
       " 456,\n",
       " 193,\n",
       " 1648,\n",
       " 523,\n",
       " 2118,\n",
       " 512,\n",
       " 1836,\n",
       " 486,\n",
       " 251,\n",
       " 859,\n",
       " 947,\n",
       " 872,\n",
       " 2050,\n",
       " 1633,\n",
       " 2351,\n",
       " 733,\n",
       " 2212,\n",
       " 2251,\n",
       " 1830,\n",
       " 1597,\n",
       " 1263,\n",
       " 912,\n",
       " 617,\n",
       " 913,\n",
       " 246,\n",
       " 1425,\n",
       " 187,\n",
       " 1721,\n",
       " 1526,\n",
       " 16,\n",
       " 1852,\n",
       " 438,\n",
       " 1297,\n",
       " 951,\n",
       " 583,\n",
       " 1894,\n",
       " 221,\n",
       " 575,\n",
       " 568,\n",
       " 2220,\n",
       " 1943,\n",
       " 1061,\n",
       " 367,\n",
       " 429,\n",
       " 372,\n",
       " 975,\n",
       " 1780,\n",
       " 644,\n",
       " 2396,\n",
       " 580,\n",
       " 1822,\n",
       " 1360,\n",
       " 230,\n",
       " 1911,\n",
       " 494,\n",
       " 1658,\n",
       " 1084,\n",
       " 323,\n",
       " 447,\n",
       " 2221,\n",
       " 1144,\n",
       " 2274,\n",
       " 445,\n",
       " 653,\n",
       " 529,\n",
       " 1598,\n",
       " 493,\n",
       " 1401,\n",
       " 1949,\n",
       " 483,\n",
       " 1869,\n",
       " 655,\n",
       " 2459,\n",
       " 830,\n",
       " 1402,\n",
       " 1862,\n",
       " 1560,\n",
       " 721,\n",
       " 2051,\n",
       " 1722,\n",
       " 1198,\n",
       " 1429,\n",
       " 249,\n",
       " 2329,\n",
       " 393,\n",
       " 1472,\n",
       " 711,\n",
       " 295,\n",
       " 1108,\n",
       " 1612,\n",
       " 1477,\n",
       " 2383,\n",
       " 1660,\n",
       " 1534,\n",
       " 1712,\n",
       " 834,\n",
       " 1455,\n",
       " 200,\n",
       " 1289,\n",
       " 451,\n",
       " 1875,\n",
       " 207,\n",
       " 190,\n",
       " 1362,\n",
       " 1371,\n",
       " 2289,\n",
       " 789,\n",
       " 1321,\n",
       " 1703,\n",
       " 1674,\n",
       " 1692,\n",
       " 1844,\n",
       " 2479,\n",
       " 82,\n",
       " 1354,\n",
       " 1957,\n",
       " 383,\n",
       " 2429,\n",
       " 1646,\n",
       " 2142,\n",
       " 1728,\n",
       " 2303,\n",
       " 751,\n",
       " 785,\n",
       " 1790,\n",
       " 1133,\n",
       " 1088,\n",
       " 1247,\n",
       " 1868,\n",
       " 1209,\n",
       " 1826,\n",
       " 1691,\n",
       " 1106,\n",
       " 2168,\n",
       " 2241,\n",
       " 1197,\n",
       " 2355,\n",
       " 1906,\n",
       " 692,\n",
       " 849,\n",
       " 1853,\n",
       " 141,\n",
       " 205,\n",
       " 812,\n",
       " 2201,\n",
       " 1839,\n",
       " 1848,\n",
       " 818,\n",
       " 865,\n",
       " 1035,\n",
       " 1909,\n",
       " 1902,\n",
       " 738,\n",
       " 333,\n",
       " 1874,\n",
       " 1813,\n",
       " 2297,\n",
       " 33,\n",
       " 2425,\n",
       " 1661,\n",
       " 1461,\n",
       " 1811,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c963bea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116,\n",
       " 134,\n",
       " 308,\n",
       " 339,\n",
       " 3,\n",
       " 191,\n",
       " 423,\n",
       " 338,\n",
       " 302,\n",
       " 362,\n",
       " 376,\n",
       " 171,\n",
       " 321,\n",
       " 156,\n",
       " 428,\n",
       " 268,\n",
       " 293,\n",
       " 194,\n",
       " 325,\n",
       " 76,\n",
       " 292,\n",
       " 236,\n",
       " 203,\n",
       " 100,\n",
       " 140,\n",
       " 148,\n",
       " 240,\n",
       " 364,\n",
       " 139,\n",
       " 43,\n",
       " 391,\n",
       " 283,\n",
       " 444,\n",
       " 48,\n",
       " 220,\n",
       " 402,\n",
       " 495,\n",
       " 92,\n",
       " 381,\n",
       " 73,\n",
       " 252,\n",
       " 375,\n",
       " 59,\n",
       " 434,\n",
       " 417,\n",
       " 132,\n",
       " 397,\n",
       " 144,\n",
       " 427,\n",
       " 479,\n",
       " 291,\n",
       " 58,\n",
       " 347,\n",
       " 89,\n",
       " 52,\n",
       " 154,\n",
       " 84,\n",
       " 87,\n",
       " 195,\n",
       " 424,\n",
       " 482,\n",
       " 259,\n",
       " 377,\n",
       " 324,\n",
       " 305,\n",
       " 36,\n",
       " 164,\n",
       " 463,\n",
       " 290,\n",
       " 301,\n",
       " 88,\n",
       " 95,\n",
       " 13,\n",
       " 342,\n",
       " 60,\n",
       " 173,\n",
       " 404,\n",
       " 68,\n",
       " 411,\n",
       " 416,\n",
       " 24,\n",
       " 146,\n",
       " 26,\n",
       " 329,\n",
       " 189,\n",
       " 300,\n",
       " 178,\n",
       " 12,\n",
       " 165,\n",
       " 65,\n",
       " 449,\n",
       " 387,\n",
       " 368,\n",
       " 288,\n",
       " 216,\n",
       " 30,\n",
       " 208,\n",
       " 136,\n",
       " 118,\n",
       " 465,\n",
       " 142,\n",
       " 420,\n",
       " 255,\n",
       " 408,\n",
       " 281,\n",
       " 179,\n",
       " 390,\n",
       " 235,\n",
       " 201,\n",
       " 232,\n",
       " 109,\n",
       " 326,\n",
       " 320,\n",
       " 488,\n",
       " 63,\n",
       " 273,\n",
       " 222,\n",
       " 241,\n",
       " 137,\n",
       " 275,\n",
       " 262,\n",
       " 307,\n",
       " 61,\n",
       " 374,\n",
       " 72,\n",
       " 448,\n",
       " 166,\n",
       " 353,\n",
       " 248,\n",
       " 454,\n",
       " 57,\n",
       " 101,\n",
       " 401,\n",
       " 238,\n",
       " 67,\n",
       " 441,\n",
       " 117,\n",
       " 27,\n",
       " 344,\n",
       " 224,\n",
       " 168,\n",
       " 299,\n",
       " 340,\n",
       " 352,\n",
       " 229,\n",
       " 373,\n",
       " 108,\n",
       " 93,\n",
       " 199,\n",
       " 50,\n",
       " 162,\n",
       " 192,\n",
       " 97,\n",
       " 357,\n",
       " 243,\n",
       " 276,\n",
       " 271,\n",
       " 227,\n",
       " 456,\n",
       " 193,\n",
       " 486,\n",
       " 251,\n",
       " 246,\n",
       " 187,\n",
       " 16,\n",
       " 438,\n",
       " 221,\n",
       " 367,\n",
       " 429,\n",
       " 372,\n",
       " 230,\n",
       " 494,\n",
       " 323,\n",
       " 447,\n",
       " 445,\n",
       " 493,\n",
       " 483,\n",
       " 249,\n",
       " 393,\n",
       " 295,\n",
       " 200,\n",
       " 451,\n",
       " 207,\n",
       " 190,\n",
       " 82,\n",
       " 383,\n",
       " 141,\n",
       " 205,\n",
       " 333,\n",
       " 33,\n",
       " 159,\n",
       " 450,\n",
       " 160,\n",
       " 412,\n",
       " 306,\n",
       " 55,\n",
       " 311,\n",
       " 385,\n",
       " 155,\n",
       " 261,\n",
       " 215,\n",
       " 37,\n",
       " 120,\n",
       " 23,\n",
       " 384,\n",
       " 147,\n",
       " 105,\n",
       " 284,\n",
       " 260,\n",
       " 185,\n",
       " 22,\n",
       " 315,\n",
       " 474,\n",
       " 111,\n",
       " 403,\n",
       " 462,\n",
       " 239,\n",
       " 398,\n",
       " 355,\n",
       " 18,\n",
       " 452,\n",
       " 358,\n",
       " 219,\n",
       " 124,\n",
       " 341,\n",
       " 370,\n",
       " 317,\n",
       " 96,\n",
       " 484,\n",
       " 157,\n",
       " 5,\n",
       " 437,\n",
       " 413,\n",
       " 69,\n",
       " 337,\n",
       " 213,\n",
       " 83,\n",
       " 396,\n",
       " 212,\n",
       " 86,\n",
       " 169,\n",
       " 409,\n",
       " 8,\n",
       " 432,\n",
       " 380,\n",
       " 112,\n",
       " 214,\n",
       " 468,\n",
       " 425,\n",
       " 316,\n",
       " 35,\n",
       " 39,\n",
       " 9,\n",
       " 312,\n",
       " 218,\n",
       " 469,\n",
       " 327,\n",
       " 478,\n",
       " 471,\n",
       " 234,\n",
       " 44,\n",
       " 107,\n",
       " 29,\n",
       " 298,\n",
       " 263,\n",
       " 415,\n",
       " 331,\n",
       " 53,\n",
       " 78,\n",
       " 41,\n",
       " 102,\n",
       " 7,\n",
       " 153,\n",
       " 332,\n",
       " 279,\n",
       " 389,\n",
       " 247,\n",
       " 314,\n",
       " 40,\n",
       " 196,\n",
       " 322,\n",
       " 464,\n",
       " 161,\n",
       " 163,\n",
       " 459,\n",
       " 45,\n",
       " 77,\n",
       " 461,\n",
       " 359,\n",
       " 204,\n",
       " 143,\n",
       " 265,\n",
       " 71,\n",
       " 172,\n",
       " 125,\n",
       " 458,\n",
       " 228,\n",
       " 433,\n",
       " 419,\n",
       " 334,\n",
       " 225,\n",
       " 28,\n",
       " 182,\n",
       " 492,\n",
       " 115,\n",
       " 330,\n",
       " 131,\n",
       " 328,\n",
       " 231,\n",
       " 407,\n",
       " 489,\n",
       " 145,\n",
       " 121,\n",
       " 272,\n",
       " 349,\n",
       " 85,\n",
       " 286,\n",
       " 47,\n",
       " 267,\n",
       " 336,\n",
       " 123,\n",
       " 475,\n",
       " 266,\n",
       " 360,\n",
       " 431,\n",
       " 10,\n",
       " 477,\n",
       " 42,\n",
       " 453,\n",
       " 335,\n",
       " 481,\n",
       " 56,\n",
       " 277,\n",
       " 310,\n",
       " 280,\n",
       " 209,\n",
       " 270,\n",
       " 51,\n",
       " 128,\n",
       " 237,\n",
       " 343,\n",
       " 354,\n",
       " 282,\n",
       " 289,\n",
       " 64,\n",
       " 470,\n",
       " 394,\n",
       " 365,\n",
       " 366,\n",
       " 80,\n",
       " 460,\n",
       " 217,\n",
       " 150,\n",
       " 114,\n",
       " 149,\n",
       " 138,\n",
       " 103,\n",
       " 369,\n",
       " 32,\n",
       " 223,\n",
       " 250,\n",
       " 351,\n",
       " 473,\n",
       " 443,\n",
       " 180,\n",
       " 206,\n",
       " 70,\n",
       " 49,\n",
       " 392,\n",
       " 421,\n",
       " 21,\n",
       " 15,\n",
       " 75,\n",
       " 440,\n",
       " 285,\n",
       " 491,\n",
       " 1,\n",
       " 175,\n",
       " 158,\n",
       " 256,\n",
       " 170,\n",
       " 152,\n",
       " 34,\n",
       " 400,\n",
       " 274,\n",
       " 257,\n",
       " 177,\n",
       " 66,\n",
       " 129,\n",
       " 181,\n",
       " 466,\n",
       " 31,\n",
       " 294,\n",
       " 378,\n",
       " 130,\n",
       " 348,\n",
       " 184,\n",
       " 74,\n",
       " 110,\n",
       " 436,\n",
       " 406,\n",
       " 395,\n",
       " 94,\n",
       " 379,\n",
       " 244]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test_filtered = [idx for idx in idx_test if idx < len(data.y)]\n",
    "idx_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f54ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    0,    0,  ..., 2478, 2478, 2479],\n",
       "         [1084, 1104, 1288,  ...,  931,  933,  999]]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "edge_index = from_scipy_sparse_matrix(adj)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371c6461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 1984)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits['idx_train']), len(splits['idx_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf3c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(x=torch.tensor(feat, dtype=torch.float),\n",
    "            edge_index=edge_index[0],\n",
    "            y=torch.tensor(labels, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import APPNP\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.appnp = APPNP(K=100, alpha=0.8, dropout=0.6, cached=True, add_self_loops=True, normalize=True)\n",
    "        self.conv1 = SAGEConv(num_node_features, num_hidden)\n",
    "        self.conv2 = SAGEConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.message_passing(x, edge_index, self.conv1)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.message_passing(x, edge_index, self.conv2)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def message_passing(self, x, edge_index, conv):\n",
    "        # Perform message passing with the given convolutional layer\n",
    "        return conv(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3e5f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 2, 0, 1, 2, 2, 3, 1, 2, 2, 2, 0, 2, 1, 1, 3, 1, 2, 1, 0, 4, 2, 1,\n",
       "        3, 2, 2, 3, 1, 2, 6, 2, 5, 2, 2, 3, 0, 1, 0, 2, 5, 3, 6, 4, 2, 2, 3, 2,\n",
       "        3, 6, 4, 4, 1, 2, 3, 6, 2, 1, 2, 5, 2, 1, 5, 5, 0, 3, 2, 2, 6, 2, 1, 2,\n",
       "        4, 3, 3, 3, 2, 3, 2, 6, 1, 0, 1, 0, 1, 3, 0, 2, 2, 1, 0, 6, 6, 6, 2, 1,\n",
       "        6, 3, 3, 2, 3, 2, 6, 3, 1, 3, 0, 3, 0, 2, 0, 3, 6, 4, 1, 2, 6, 3, 2, 1,\n",
       "        2, 4, 1, 2, 0, 0, 2, 2, 4, 2, 6, 1, 6, 3, 3, 2, 3, 1, 4, 0, 6, 5, 3, 5,\n",
       "        1, 2, 0, 1, 1, 3, 3, 2, 3, 3, 3, 1, 0, 3, 3, 1, 1, 4, 4, 1, 2, 1, 3, 2,\n",
       "        2, 6, 1, 2, 3, 6, 6, 3, 6, 2, 4, 2, 5, 0, 3, 6, 0, 6, 3, 6, 2, 1, 2, 4,\n",
       "        1, 3, 1, 0, 2, 2, 2, 2, 3, 2, 0, 0, 0, 2, 4, 2, 0, 1, 2, 2, 4, 0, 3, 2,\n",
       "        3, 2, 2, 4, 1, 6, 6, 6, 4, 6, 6, 1, 3, 3, 2, 1, 6, 6, 2, 3, 1, 0, 3, 2,\n",
       "        2, 2, 2, 6, 2, 4, 4, 5, 4, 2, 2, 1, 2, 0, 4, 1, 1, 3, 2, 1, 2, 5, 2, 2,\n",
       "        0, 4, 6, 1, 2, 3, 2, 5, 2, 4, 4, 6, 1, 5, 2, 5, 6, 1, 2, 3, 1, 3, 1, 4,\n",
       "        2, 2, 1, 1, 3, 2, 3, 3, 2, 4, 6, 0, 3, 5, 1, 2, 1, 6, 4, 0, 0, 1, 1, 0,\n",
       "        1, 3, 4, 2, 2, 6, 0, 1, 2, 3, 6, 0, 1, 0, 0, 1, 6, 1, 5, 2, 6, 2, 2, 6,\n",
       "        6, 2, 1, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 1, 4, 2, 2, 2, 6, 5, 6, 2, 3,\n",
       "        2, 3, 2, 2, 3, 4, 2, 4, 0, 2, 1, 2, 2, 1, 6, 5, 2, 6, 2, 1, 1, 0, 0, 0,\n",
       "        1, 5, 3, 0, 3, 6, 2, 2, 4, 3, 3, 0, 4, 1, 0, 4, 6, 4, 6, 1, 0, 1, 6, 3,\n",
       "        0, 6, 2, 6, 5, 0, 3, 6, 3, 1, 3, 4, 3, 3, 2, 2, 1, 6, 2, 5, 1, 2, 2, 2,\n",
       "        0, 6, 6, 0, 2, 0, 0, 6, 2, 6, 3, 6, 4, 4, 1, 2, 2, 6, 1, 5, 5, 6, 2, 3,\n",
       "        4, 2, 5, 0, 2, 1, 6, 2, 6, 4, 1, 2, 0, 2, 1, 5, 6, 0, 3, 6, 5, 3, 3, 0,\n",
       "        4, 4, 3, 1, 2, 2, 0, 2, 4, 1, 0, 2, 2, 6, 2, 6], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9368270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/8\n",
      "Validation Accuracy: 0.1935483870967742 at Epoch 0\n",
      "Fold 2/8\n",
      "Validation Accuracy: 0.27419354838709675 at Epoch 0\n",
      "Fold 3/8\n",
      "Validation Accuracy: 0.16129032258064516 at Epoch 0\n",
      "Fold 4/8\n",
      "Validation Accuracy: 0.16129032258064516 at Epoch 0\n",
      "Fold 5/8\n",
      "Validation Accuracy: 0.22580645161290322 at Epoch 0\n",
      "Fold 6/8\n",
      "Validation Accuracy: 0.24193548387096775 at Epoch 0\n",
      "Fold 7/8\n",
      "Validation Accuracy: 0.0967741935483871 at Epoch 0\n",
      "Fold 8/8\n",
      "Validation Accuracy: 0.1935483870967742 at Epoch 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_test_acc = 0.0\n",
    "best_test_predictions = None\n",
    "\n",
    "k_folds = 8\n",
    "kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold, (train_fold_idx, val_idx) in enumerate(kf.split(data.y)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = GraphSage(num_node_features=data.x.shape[1],\n",
    "                      num_hidden=128,\n",
    "                      num_classes=(data.y.max() + 1).item()\n",
    "                      ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.01, weight_decay=5e-3)\n",
    "\n",
    "    best_epoch = 0\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[train_fold_idx], data.y[train_fold_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            val_acc = pred[val_idx].eq(data.y[val_idx]).sum().item() / len(val_idx)\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = model.state_dict().copy()\n",
    "\n",
    "    print(f'Validation Accuracy: {val_acc} at Epoch {best_epoch}')\n",
    "\n",
    "    # Load the best model for inference\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(data).argmax(dim=1)\n",
    "        acc = pred[idx_test_filtered].eq(data.y[idx_test_filtered]).sum().item() / len(idx_test_filtered)\n",
    "      #  print(f\"Test Accuracy: {acc}\")\n",
    "        if acc > best_test_acc:\n",
    "            best_test_acc = acc\n",
    "            best_test_predictions = pred[idx_test]\n",
    "          #  np.savetxt('submissiontest.txt', best_test_predictions, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce21b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/15\n",
      "Validation Accuracy: 0.35294117647058826 at Epoch 70\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 70\n",
      "Fold 2/15\n",
      "Validation Accuracy: 0.21212121212121213 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 3/15\n",
      "Validation Accuracy: 0.24242424242424243 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 4/15\n",
      "Validation Accuracy: 0.2727272727272727 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 5/15\n",
      "Validation Accuracy: 0.18181818181818182 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 6/15\n",
      "Validation Accuracy: 0.12121212121212122 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 7/15\n",
      "Validation Accuracy: 0.24242424242424243 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 8/15\n",
      "Validation Accuracy: 0.30303030303030304 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 9/15\n",
      "Validation Accuracy: 0.12121212121212122 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 10/15\n",
      "Validation Accuracy: 0.21212121212121213 at Epoch 0\n",
      "Test Accuracy: 0.3012345679012346\n",
      "Best Validation Accuracy: 0.47058823529411764 at Epoch 0\n",
      "Fold 11/15\n",
      "Validation Accuracy: 0.09090909090909091 at Epoch 0\n",
      "Test Accuracy: 0.1506172839506173\n",
      "Best Validation Accuracy: 0.6060606060606061 at Epoch 0\n",
      "Fold 12/15\n",
      "Validation Accuracy: 0.2727272727272727 at Epoch 0\n",
      "Test Accuracy: 0.1506172839506173\n",
      "Best Validation Accuracy: 0.6060606060606061 at Epoch 0\n",
      "Fold 13/15\n",
      "Validation Accuracy: 0.15151515151515152 at Epoch 0\n",
      "Test Accuracy: 0.1506172839506173\n",
      "Best Validation Accuracy: 0.6060606060606061 at Epoch 0\n",
      "Fold 14/15\n",
      "Validation Accuracy: 0.24242424242424243 at Epoch 0\n",
      "Test Accuracy: 0.1506172839506173\n",
      "Best Validation Accuracy: 0.6060606060606061 at Epoch 0\n",
      "Fold 15/15\n",
      "Validation Accuracy: 0.18181818181818182 at Epoch 0\n",
      "Test Accuracy: 0.1506172839506173\n",
      "Best Validation Accuracy: 0.6060606060606061 at Epoch 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_test_acc = 0.0\n",
    "best_test_predictions = None\n",
    "\n",
    "k_folds = 15\n",
    "kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold, (train_fold_idx, val_idx) in enumerate(kf.split(data.y)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = GraphSage(num_node_features=data.x.shape[1],\n",
    "                      num_hidden=128,\n",
    "                      num_classes=(data.y.max() + 1).item()\n",
    "                      ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3.5, weight_decay=5e-3)\n",
    "\n",
    "    best_epoch = 0\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[train_fold_idx], data.y[train_fold_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            val_acc = pred[val_idx].eq(data.y[val_idx]).sum().item() / len(val_idx)\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = model.state_dict().copy()\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc} at Epoch {best_epoch}\")\n",
    "\n",
    "    # Load the best model for inference\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(data).argmax(dim=1)\n",
    "        acc = pred[idx_test_filtered].eq(data.y[idx_test_filtered]).sum().item() / len(idx_test_filtered)\n",
    "        print(f\"Test Accuracy: {acc}\")\n",
    "        if acc > best_test_acc:\n",
    "            best_test_acc = acc\n",
    "            best_test_predictions = pred[idx_test]\n",
    "          #  np.savetxt('submissiontest.txt', best_test_predictions, fmt='%d')\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc} at Epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4576f3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "        2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 1, 1, 1, 2, 2, 6, 1, 2, 0, 2, 1, 6, 2, 0,\n",
       "        2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 0, 3, 0, 0, 2, 2, 6, 2, 2, 2, 2, 3, 1,\n",
       "        2, 2, 2, 2, 3, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 1, 2, 2, 2, 2, 2, 1,\n",
       "        2, 2, 1, 1, 2, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 1, 2, 6, 3, 2, 1, 2, 2, 1,\n",
       "        6, 2, 2, 2, 1, 0, 2, 0, 4, 6, 2, 2, 1, 2, 0, 2, 3, 2, 2, 1, 2, 6, 3, 2,\n",
       "        1, 2, 0, 1, 0, 6, 0, 2, 2, 2, 2, 2, 1, 2, 0, 0, 2, 1, 6, 2, 2, 2, 6, 3,\n",
       "        6, 2, 6, 2, 1, 3, 2, 2, 2, 2, 1, 6, 3, 1, 2, 0, 2, 2, 1, 4, 2, 1, 2, 6,\n",
       "        2, 2, 4, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 6,\n",
       "        2, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 3, 2, 2, 0, 0, 0,\n",
       "        2, 6, 2, 2, 0, 2, 1, 2, 0, 1, 2, 2, 2, 2, 6, 2, 2, 2, 6, 0, 2, 6, 2, 2,\n",
       "        2, 6, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 0, 6, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 6, 4, 2, 2, 1, 2, 1, 2, 6, 2, 2, 2, 0, 2, 2, 2, 1, 6, 2, 3, 2, 2,\n",
       "        6, 2, 2, 3, 2, 2, 2, 3, 0, 6, 2, 2, 2, 2, 0, 6, 3, 2, 2, 0, 3, 2, 2, 2,\n",
       "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 6, 6, 2, 3, 0, 0, 2, 2, 4, 2, 0,\n",
       "        2, 0, 6, 2, 2, 2, 3, 6, 2, 2, 2, 1, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6,\n",
       "        2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[idx_test_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb88dc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 2, 0, 1, 2, 2, 3, 1, 2, 2, 2, 0, 2, 1, 1, 3, 1, 2, 1, 0, 4, 2, 1,\n",
       "        3, 2, 2, 3, 1, 2, 6, 2, 5, 2, 2, 3, 0, 1, 0, 2, 5, 3, 6, 4, 2, 2, 3, 2,\n",
       "        3, 6, 4, 4, 1, 2, 3, 6, 2, 1, 2, 5, 2, 1, 5, 5, 0, 3, 2, 2, 6, 2, 1, 2,\n",
       "        4, 3, 3, 3, 2, 3, 2, 6, 1, 0, 1, 0, 1, 3, 0, 2, 2, 1, 0, 6, 6, 6, 2, 1,\n",
       "        6, 3, 3, 2, 3, 2, 6, 3, 1, 3, 0, 3, 0, 2, 0, 3, 6, 4, 1, 2, 6, 3, 2, 1,\n",
       "        2, 4, 1, 2, 0, 0, 2, 2, 4, 2, 6, 1, 6, 3, 3, 2, 3, 1, 4, 0, 6, 5, 3, 5,\n",
       "        1, 2, 0, 1, 1, 3, 3, 2, 3, 3, 3, 1, 0, 3, 3, 1, 1, 4, 4, 1, 2, 1, 3, 2,\n",
       "        2, 6, 1, 2, 3, 6, 6, 3, 6, 2, 4, 2, 5, 0, 3, 6, 0, 6, 3, 6, 2, 1, 2, 4,\n",
       "        1, 3, 1, 0, 2, 2, 2, 2, 3, 2, 0, 0, 0, 2, 4, 2, 0, 1, 2, 2, 4, 0, 3, 2,\n",
       "        3, 2, 2, 4, 1, 6, 6, 6, 4, 6, 6, 1, 3, 3, 2, 1, 6, 6, 2, 3, 1, 0, 3, 2,\n",
       "        2, 2, 2, 6, 2, 4, 4, 5, 4, 2, 2, 1, 2, 0, 4, 1, 1, 3, 2, 1, 2, 5, 2, 2,\n",
       "        0, 4, 6, 1, 2, 3, 2, 5, 2, 4, 4, 6, 1, 5, 2, 5, 6, 1, 2, 3, 1, 3, 1, 4,\n",
       "        2, 2, 1, 1, 3, 2, 3, 3, 2, 4, 6, 0, 3, 5, 1, 2, 1, 6, 4, 0, 0, 1, 1, 0,\n",
       "        1, 3, 4, 2, 2, 6, 0, 1, 2, 3, 6, 0, 1, 0, 0, 1, 6, 1, 5, 2, 6, 2, 2, 6,\n",
       "        6, 2, 1, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 1, 4, 2, 2, 2, 6, 5, 6, 2, 3,\n",
       "        2, 3, 2, 2, 3, 4, 2, 4, 0, 2, 1, 2, 2, 1, 6, 5, 2, 6, 2, 1, 1, 0, 0, 0,\n",
       "        1, 5, 3, 0, 3, 6, 2, 2, 4, 3, 3, 0, 4, 1, 0, 4, 6, 4, 6, 1, 0, 1, 6, 3,\n",
       "        0, 6, 2, 6, 5, 0, 3, 6, 3, 1, 3, 4, 3, 3, 2, 2, 1, 6, 2, 5, 1, 2, 2, 2,\n",
       "        0, 6, 6, 0, 2, 0, 0, 6, 2, 6, 3, 6, 4, 4, 1, 2, 2, 6, 1, 5, 5, 6, 2, 3,\n",
       "        4, 2, 5, 0, 2, 1, 6, 2, 6, 4, 1, 2, 0, 2, 1, 5, 6, 0, 3, 6, 5, 3, 3, 0,\n",
       "        4, 4, 3, 1, 2, 2, 0, 2, 4, 1, 0, 2, 2, 6, 2, 6], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a026f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([405])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[idx_test_filtered].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd096fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model = None\n",
    "best_test_acc = 0.0\n",
    "best_test_predictions = None\n",
    "\n",
    "k_folds = 10\n",
    "kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold, (train_fold_idx, val_idx) in enumerate(kf.split(data.y)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = GAT(num_node_features=data.x.shape[1],\n",
    "                      num_hidden=100,\n",
    "                      num_classes=(data.y.max() + 1).item()\n",
    "                      ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2.0, weight_decay=5e-3)\n",
    "\n",
    "    best_epoch = 0\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss =  F.nll_loss(out[train_idx], data.y[train_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            val_acc = pred[val_idx].eq(data.y[val_idx]).sum().item() / len(val_idx)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = model.state_dict().copy()\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc} at Epoch {best_epoch}\")\n",
    "\n",
    "    # Load the best model for inference\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if acc > best_test_acc:\n",
    "            best_test_acc = acc\n",
    "            best_test_predictions = pred[idx_test]\n",
    "            np.savetxt('submissiontest.txt', best_test_predictions, fmt='%d')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('graphsage')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1cd7fe2092f8558eaeec7a344d6c72cd3491c14a7d06f3a2798a65abb497b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
